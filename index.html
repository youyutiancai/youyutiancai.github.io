<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  
    <title>Yuqi Zhou - About me</title>
    <meta name="description" content="Webpage of Yuqi Zhou, Ph.D Candidate">
  
    <link rel="stylesheet" href="./css/bootstrap.min.css">
    <link rel="stylesheet" href="./css/main.css">
    <link rel="canonical" href="https://youyutiancai.github.io/">
  </head>
  
  
    <body>
      <div class="container">
        <div class="header clearfix">
            <a href="/" title="Homepage"><h3 class="text-muted">Yuqi Zhou</h3></a>
            <nav>
                <ul class="nav nav-pills">
                    <li class="active">
                        <a href="/">About me</a>
                    </li>
                    <li>
                        <a href="./publications.html">Publications</a>
                    </li>
                    <li>
                        <a href="./contact.html">Contact</a>
                    </li>
                </ul>
            </nav>
        </div>
  
  
        <div class="row">
          <div class="col-lg-12 blog-main">
            <h1>About me</h1>
  <div class="col-md-9">
      <p lang="en">
          I am currently a Computer Science PhD candidate at <a href="https://www.cs.purdue.edu/">Purdue University</a>, 
          advised by Professor <a href="https://www.cs.purdue.edu/homes/popescu/">Voicu Popescu</a>.
          I am interested in extended reality (XR), especially haptics for virtual reality (VR). My take on the future of VR haptics is 
          an immersive world in which the user can touch any virtual object within their reach—a lot like in the 
          <a href="https://en.wikipedia.org/wiki/Ready_Player_One_(film)">Ready Player One</a> movie. I have taken a first step in this 
          direction: I designed and built an Encountered-Type Haptic Device (ETHD) in the form of a Cartesian robot. I am also a big fan 
          of wearable haptic devices, on which I plan to work in the immediate future. Other than VR haptics, I am also interested in 
          other XR fields, such as cloud VR and collaborative augmented reality (AR). Here is my 
          <a href="contents/resume_YuqiZhou.pdf">CV</a>.
</div>
  
  <div class="col-md-3">
      <img src="images/Yuqi.jpg" alt="Yuqi Zhou" class="img-responsive img-rounded">
  </div>
  
  <div class="clearfix"></div>
  <hr>
  
  <h1>Research projects</h1>
  
  <!-- ETHD -->
  <div class="col-md-5">
      <a href="./images/ETHD.png" title="EHTD"><img src="images/ETHD.png" alt="ETHD design" class="img-responsive"></a>
  </div>
  <div class="col-md-7">
      <h2><a href="https://github.com/youyutiancai/ETHD_Cartesian/tree/main">VR haptics with ETHD</a></h2>
      <p lang="en"> 
        One can allow the user to touch any virtual object by building a high-fidelity physical replica of the virtual environment—but 
        that is impractical. A better approach is to try to alleviate the differences between the virtual and the physical world. One 
        option is to change the physical world to align it locally with the part of the virtual world with which the user is about to 
        interact. For this I built an ETHD, a Cartesian robot that moves a physical object to align it with the virtual object the user 
        is about to touch. Another option is to change the virtual world to align it with the physical world. I have developed physical 
        and virtual redirection algorithms that cooperate to allow the user to touch stationary or dynamic virtual objects of varying 
        shapes with perfect synchronization of the virtual and physical contacts. This provides the user with safe and believable haptic 
        feedback. The image shows my ETHD providing haptic feedback to the user.
        <!--See the <a href="/publications">publications page</a> for more details.-->
      </p>
  </div>
  
  <div class="clearfix"></div>
  <div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div>
  <div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div>
  <div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div>
  <div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div>
  <!-- CloVR -->
  <div class="col-md-5">
      <a href="./images/CloVR.png" title="CloVR"><img src="images/CloVR.png" alt="CloVR" class="img-responsive"></a>
  </div>
  <div class="col-md-7">
      <h2>CloVR: Fast-Startup Low-Latency Cloud VR</h2>
      <p lang="en">We now have amazing self-contained XR headsets with on-board tracking, rendering, and display, delivering to the user 
        completely untethered VR experiences. One challenge is to increase the complexity of the virtual environments that can be rendered
        on these headsets. I am working on a distributed VR system, dubbed CloVR, which partitions the rendering load between the server 
        and the client, shielding the client from the full complexity of the virtual environment. The images show how CloVR downloads a 
        visually complete and functionally virtual environment much faster than a conventional download of visible objects.
      </p>
  </div>

  <div class="clearfix"></div>
  <div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div>
  <div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div>
  <div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div>
  <div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div>
  <div class="col-md-5">
    <a href="./images/attention_guidance.png" title="attention_guidance"><img src="images/attention_guidance.png" alt="attention_guidance" class="img-responsive"></a>
  </div>
  <div class="col-md-7">
      <h2>Co-located AR Attention Guidance</h2>
      <p lang="en">Consider two collaborators, e.g., an instructor and a student, standing side-by-side, looking at the same 3D scene. 
        I developed a method that allows the instructor to point out an element of the scene to the student using tablets or phones. 
        The instructor circles the target on their display and a circle appears around the target on the student's display. Then the 
        student's display turns transparent, i.e., the live video it shows grows similar to what the student would see if the display 
        were actually made of glass. The transparent display gives to the student accurate directional guidance to the target, helping 
        them find it with the naked eye. The image shows the transparent student display pointing to a window of the far away building.
      </p>
  </div>

  <!-- <div class="clearfix"></div>
  <div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div>
  <div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div>
  <div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div>
  <div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div><div class="col-md-12"></div>
  <div class="col-md-4">
    <a href="./images/redirection.png" title="redirection"><img src="images/redirection.png" alt="redirection" class="img-responsive"></a>
  </div>
  <div class="col-md-8>
      <h2>Passive VR Haptics with Handheld Stick and Redirection</h2>
      <p lang="en">Unlike a robot that can move objects around, a stationary object can only provide passive haptic feedback of virtual objects that are perfectly aligned.
        As a result, the same setup of a physical environment can only be used to offer haptic feedback for a specific virtual setup. The redirection algorithm can solve
        this problem by retargeting the user body or handheld stick (in our case) gradually when the user is approaching the physical object. I came up with a noval algorithm
        that not only ensures the synchronized virtual and physical contact at the end, but also enables shape mapping with smooth transition. 
        That is, a physical object can provide haptic feedback for virtual objects in different shapes.
      </p>
  </div> -->
  
  
          </div>
        </div>
  
        <footer class="footer">
    <!-- <p>
      © 2023 Mathieu GAILLARD.
      <a href="/feed.xml">
        <img src="/images/rss-icon.svg" alt="RSS" height="14">
      </a>
  
      <a href="https://github.com/mgaillard" title="Go to mgaillard's GitHub profile">
        <img src="/images/github.png" alt="GitHub" height="14">
      </a>
  
      <a href="https://orcid.org/0000-0003-0840-5654" title="Go to Mathieu Gaillard's ORCID profile">
        <img src="/images/orcid_logo.svg" alt="ORCID" height="14">
      </a>
  
      <a href="https://www.linkedin.com/in/mathieu-gaillard-9b6386a8" title="Go to Mathieu Gaillard's LinkedIn profile">
          <img src="/images/linkedin.png" alt="LinkedIn" height="14">
      </a>
    </p> -->
  </footer>
      </div> <!-- /container -->
    
  
  
  </body></html>
